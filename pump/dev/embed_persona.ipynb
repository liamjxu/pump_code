{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "\n",
    "# persona_val_path = \"opinions_qa/persona_val/American_Trends_Panel_W34/date0828_personas_American_Trends_Panel_W34_testonly_haiku.json\"\n",
    "persona_val_path = \"opinions_qa/persona_val/American_Trends_Panel_W34/date0831_personas_full_haiku_known_test.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ubuntu/code/pump_post_midterm/pump\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2537\n"
     ]
    }
   ],
   "source": [
    "with open(persona_val_path, 'r') as f:\n",
    "    p_vals = json.load(f)\n",
    "print(len(list(p_vals.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 253\n",
      "train: 2284\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from src.utils import get_file_from_s3\n",
    "random.seed(42)\n",
    "\n",
    "file_key = \"American_Trends_Panel_W34\"\n",
    "resp_df = pd.read_csv(get_file_from_s3(f\"human_resp/{file_key}/responses.csv\"))\n",
    "test_user_idx = random.choices(range(len(resp_df)), k=int(len(resp_df)*0.1))\n",
    "test_resp_df = resp_df.iloc[test_user_idx]\n",
    "test_len = int(len(resp_df)*0.1)\n",
    "\n",
    "test_user_list = list(p_vals.keys())[:test_len]\n",
    "train_user_list = list(p_vals.keys())[test_len:]\n",
    "\n",
    "print('test:', len(test_user_list))\n",
    "print('train:', len(train_user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_pval_df(pvals, user_list):\n",
    "    records = []\n",
    "    for user in user_list:\n",
    "        personas = pvals[user]\n",
    "        entry = {'user': user}\n",
    "        for p in personas:\n",
    "            entry[p['name']] = p['inferred_value']\n",
    "        records.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>Safety vs. Timeliness Prioritization</th>\n",
       "      <th>Health and Nutrition Values</th>\n",
       "      <th>Personal Values on Gene Editing</th>\n",
       "      <th>Concern for Social Justice</th>\n",
       "      <th>Worldview and Belief System</th>\n",
       "      <th>Openness to Food Innovation</th>\n",
       "      <th>Trust in Science and Technology</th>\n",
       "      <th>Lifestyle Priorities</th>\n",
       "      <th>Core Personal Values</th>\n",
       "      <th>...</th>\n",
       "      <th>Attitude Towards Science</th>\n",
       "      <th>Risk Perception</th>\n",
       "      <th>Technological Outlook</th>\n",
       "      <th>Personal Health Concerns</th>\n",
       "      <th>Technological Awareness</th>\n",
       "      <th>Dietary Habits</th>\n",
       "      <th>Healthcare System Familiarity</th>\n",
       "      <th>Information Sources on GMOs</th>\n",
       "      <th>Access to Healthcare</th>\n",
       "      <th>Personal Experiences with Scientific and Technological Advancements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>prioritizes safety over speed</td>\n",
       "      <td>balanced values</td>\n",
       "      <td>values caution and preservation</td>\n",
       "      <td>low concern</td>\n",
       "      <td>conservative</td>\n",
       "      <td>resistant to innovation</td>\n",
       "      <td>low trust</td>\n",
       "      <td>prioritizes other lifestyle factors</td>\n",
       "      <td>prioritizes individual freedom</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>low risk perception</td>\n",
       "      <td>pessimistic</td>\n",
       "      <td>does not have personal health concerns</td>\n",
       "      <td>low awareness</td>\n",
       "      <td>consistently unhealthy</td>\n",
       "      <td>low familiarity</td>\n",
       "      <td>non-scientific sources</td>\n",
       "      <td>low access</td>\n",
       "      <td>negative experiences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>prioritizes safety over speed</td>\n",
       "      <td>balanced values</td>\n",
       "      <td>values caution and preservation</td>\n",
       "      <td>moderate concern</td>\n",
       "      <td>moderate</td>\n",
       "      <td>resistant to innovation</td>\n",
       "      <td>low trust</td>\n",
       "      <td>balanced priorities</td>\n",
       "      <td>prioritizes safety</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>low risk perception</td>\n",
       "      <td>pessimistic</td>\n",
       "      <td>does not have personal health concerns</td>\n",
       "      <td>low awareness</td>\n",
       "      <td>balanced</td>\n",
       "      <td>moderate familiarity</td>\n",
       "      <td>trusts non-scientific sources</td>\n",
       "      <td>low access</td>\n",
       "      <td>negative experiences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>prioritizes safety over speed</td>\n",
       "      <td>balanced values</td>\n",
       "      <td>values caution and preservation</td>\n",
       "      <td>high concern</td>\n",
       "      <td>moderate</td>\n",
       "      <td>open to innovation</td>\n",
       "      <td>high trust</td>\n",
       "      <td>prioritizes health and nutrition</td>\n",
       "      <td>prioritizes individual freedom</td>\n",
       "      <td>...</td>\n",
       "      <td>positive</td>\n",
       "      <td>low risk perception</td>\n",
       "      <td>optimistic</td>\n",
       "      <td>has personal health concerns</td>\n",
       "      <td>low awareness</td>\n",
       "      <td>balanced</td>\n",
       "      <td>moderate familiarity</td>\n",
       "      <td>trusts scientific sources</td>\n",
       "      <td>low access</td>\n",
       "      <td>positive experiences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user Safety vs. Timeliness Prioritization Health and Nutrition Values  \\\n",
       "0   13        prioritizes safety over speed             balanced values   \n",
       "1   14        prioritizes safety over speed             balanced values   \n",
       "2   15        prioritizes safety over speed             balanced values   \n",
       "\n",
       "   Personal Values on Gene Editing Concern for Social Justice  \\\n",
       "0  values caution and preservation                low concern   \n",
       "1  values caution and preservation           moderate concern   \n",
       "2  values caution and preservation               high concern   \n",
       "\n",
       "  Worldview and Belief System Openness to Food Innovation  \\\n",
       "0                conservative     resistant to innovation   \n",
       "1                    moderate     resistant to innovation   \n",
       "2                    moderate          open to innovation   \n",
       "\n",
       "  Trust in Science and Technology                 Lifestyle Priorities  \\\n",
       "0                       low trust  prioritizes other lifestyle factors   \n",
       "1                       low trust                  balanced priorities   \n",
       "2                      high trust     prioritizes health and nutrition   \n",
       "\n",
       "             Core Personal Values  ... Attitude Towards Science  \\\n",
       "0  prioritizes individual freedom  ...                 negative   \n",
       "1              prioritizes safety  ...                 negative   \n",
       "2  prioritizes individual freedom  ...                 positive   \n",
       "\n",
       "       Risk Perception Technological Outlook  \\\n",
       "0  low risk perception           pessimistic   \n",
       "1  low risk perception           pessimistic   \n",
       "2  low risk perception            optimistic   \n",
       "\n",
       "                 Personal Health Concerns Technological Awareness  \\\n",
       "0  does not have personal health concerns           low awareness   \n",
       "1  does not have personal health concerns           low awareness   \n",
       "2            has personal health concerns           low awareness   \n",
       "\n",
       "           Dietary Habits Healthcare System Familiarity  \\\n",
       "0  consistently unhealthy               low familiarity   \n",
       "1                balanced          moderate familiarity   \n",
       "2                balanced          moderate familiarity   \n",
       "\n",
       "     Information Sources on GMOs Access to Healthcare  \\\n",
       "0         non-scientific sources           low access   \n",
       "1  trusts non-scientific sources           low access   \n",
       "2      trusts scientific sources           low access   \n",
       "\n",
       "  Personal Experiences with Scientific and Technological Advancements  \n",
       "0                               negative experiences                   \n",
       "1                               negative experiences                   \n",
       "2                               positive experiences                   \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_user_pval_df(p_vals, train_user_list)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health and Nutrition Values\n",
      "Counter({'balanced values': 2225, 'values convenience and indulgence': 40, 'values health and wellness': 19})\n",
      "\n",
      "Personal Values on Gene Editing\n",
      "Counter({'values caution and preservation': 2261, 'values progress and innovation': 23})\n",
      "\n",
      "Ethical Orientation\n",
      "Counter({'low ethical concern': 2280, 'high ethical concern': 4})\n",
      "\n",
      "Attitudes Towards Genetic Engineering\n",
      "Counter({'skeptical': 2274, 'optimistic': 10})\n",
      "\n",
      "Food Motivations\n",
      "Counter({'balanced': 2283, 'pleasure-focused': 1})\n",
      "\n",
      "0.1282051282051282\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "skew_cnt = 0\n",
    "skew_personas = []\n",
    "for col in df.columns:\n",
    "    if col == 'user': continue\n",
    "    cnt = Counter(df[col].values)\n",
    "    if len(cnt.keys()) < 2: \n",
    "        skew_cnt += 1\n",
    "        skew_personas.append(col) \n",
    "        print(col)\n",
    "        print(Counter(df[col].values))\n",
    "        print()\n",
    "        continue\n",
    "    first, second = cnt.most_common(2)\n",
    "    if first[1] / second[1] > 50:\n",
    "        skew_cnt += 1\n",
    "        skew_personas.append(col) \n",
    "        print(col)\n",
    "        print(Counter(df[col].values))\n",
    "        print()\n",
    "print(skew_cnt / len(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Safety vs. Timeliness Prioritization: prioritizes safety over speed; Concern for Social Justice: moderate concern; Worldview and Belief System: moderate; Openness to Food Innovation: resistant to innovation; Trust in Science and Technology: low trust; Lifestyle Priorities: balanced priorities; Core Personal Values: prioritizes individual freedom; Ethical Concerns about Gene Editing: high ethical concerns; Personal Values Towards Genetically Modified Foods: prioritizes health and safety; Trust in Healthcare: high trust; Environmental Consciousness: low environmental consciousness; Perceived Risk of Genetically Modified Foods: low risk perception; Self-Discipline: moderate; Perception of Technological Equity: indifferent to inequality; Perception of Healthcare System Efficiency: does not see it as a problem; Health Consciousness: moderate health consciousness; Emotional Relationship with Food: neutral; Food Preferences: indifferent to food type; Scientific Literacy: moderate; Nutrition Knowledge: moderate; Perception of Gene Editing Impact: negative impact; Attitude Towards Medical Innovation: less concerned about speed of innovation; Risk Tolerance: moderate; Attitude Towards Science: negative; Risk Perception: low risk perception; Technological Outlook: neutral; Personal Health Concerns: has personal health concerns; Technological Awareness: low awareness; Dietary Habits: balanced; Healthcare System Familiarity: low familiarity; Information Sources on GMOs: trusts non-scientific sources; Access to Healthcare: low access; Personal Experiences with Scientific and Technological Advancements: negative experiences'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "su = test_user_list[0]\n",
    "\n",
    "def get_user_persona_repr(personas):\n",
    "    return '; '.join(f\"{p['name']}: {p['inferred_value']}\" for p in personas if p['name'] not in skew_personas)\n",
    "\n",
    "get_user_persona_repr(p_vals[su])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('nvidia/NV-Embed-v2')\n",
    "\n",
    "len(tok.tokenize(get_user_persona_repr(p_vals[su])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01497ae7701040ac8a166f56fc28388f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d16e855b9fa4aa7a37db88b1c4a60bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding:   0%|          | 0/127 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/ubuntu/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/51f27399af65db6ba77cf2cd43bb7b59e1b7b9e3/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# query_embeddings = model.encode(queries, instruction=query_prefix, max_length=max_length)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# passage_embeddings = model.encode(passages, instruction=passage_prefix, max_length=max_length)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 29\u001b[0m query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest encoding time:\u001b[39m\u001b[38;5;124m'\u001b[39m, toc\u001b[38;5;241m-\u001b[39mtic)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/51f27399af65db6ba77cf2cd43bb7b59e1b7b9e3/modeling_nvembed.py:390\u001b[0m, in \u001b[0;36mNVEmbedModel._do_encode\u001b[0;34m(self, prompts, batch_size, instruction, max_length, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_dict \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    389\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_kwargs_from_batch(batch_dict, instruction_lens, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 390\u001b[0m     embeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    391\u001b[0m     encoded_embeds\u001b[38;5;241m.\u001b[39mappend(embeds)\n\u001b[1;32m    392\u001b[0m encoded_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(encoded_embeds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/51f27399af65db6ba77cf2cd43bb7b59e1b7b9e3/modeling_nvembed.py:401\u001b[0m, in \u001b[0;36mNVEmbedModel.forward\u001b[0;34m(self, input_ids, attention_mask, pool_mask, return_dict)\u001b[0m\n\u001b[1;32m    398\u001b[0m autocast_ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_ctx(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m## decoder only layer\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m## latent attention layer\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_attention_model(\n\u001b[1;32m    407\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state,\n\u001b[1;32m    408\u001b[0m         pool_mask,\n\u001b[1;32m    409\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/51f27399af65db6ba77cf2cd43bb7b59e1b7b9e3/modeling_nvembed.py:146\u001b[0m, in \u001b[0;36mBidirectionalMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    136\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    137\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    138\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         use_cache,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:732\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 732\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    735\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:171\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(hidden_state)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/pump/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import time\n",
    "\n",
    "# Each query needs to be accompanied by an corresponding instruction describing the task.\n",
    "task_name_to_instruct = {\"retrieve_user_personas\": \"Given a user persona description, retrieve similar user presona descriptions\",}\n",
    "\n",
    "query_prefix = \"Instruct: \"+task_name_to_instruct[\"retrieve_user_personas\"]+\"\\nQuery: \"\n",
    "queries = [\n",
    "    get_user_persona_repr(p_vals[user]) for user in test_user_list\n",
    "]\n",
    "\n",
    "# No instruction needed for retrieval passages\n",
    "passage_prefix = \"\"\n",
    "passages = [\n",
    "    get_user_persona_repr(p_vals[user]) for user in train_user_list[:200]\n",
    "]\n",
    "\n",
    "# load model with tokenizer\n",
    "model = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True, device_map='auto')\n",
    "\n",
    "# get the embeddings\n",
    "max_length = 4096\n",
    "tic = time.time()\n",
    "# query_embeddings = model.encode(queries, instruction=query_prefix, max_length=max_length)\n",
    "# passage_embeddings = model.encode(passages, instruction=passage_prefix, max_length=max_length)\n",
    "batch_size=2\n",
    "query_embeddings = model._do_encode(queries, batch_size=batch_size, instruction=query_prefix, max_length=max_length, num_workers=32, return_numpy=True)\n",
    "toc = time.time()\n",
    "print('Test encoding time:', toc-tic)\n",
    "\n",
    "passage_embeddings = model._do_encode(passages, batch_size=batch_size, instruction=passage_prefix, max_length=max_length, num_workers=32, return_numpy=True)\n",
    "toc = time.time()\n",
    "print('Total encoding time:', toc-tic)\n",
    "\n",
    "# normalize embeddings\n",
    "# query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "# passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
    "query_embeddings = F.normalize(torch.tensor(query_embeddings), p=2, dim=1)\n",
    "passage_embeddings = F.normalize(torch.tensor(passage_embeddings), p=2, dim=1)\n",
    "\n",
    "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
    "# print(scores.tolist())\n",
    "# [[87.42692565917969, 0.462837278842926], [0.9652643203735352, 86.0372314453125]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[78.6956, 81.3270, 79.4460,  ..., 84.1884, 83.9234, 80.1651],\n",
       "        [79.3121, 82.8288, 81.6716,  ..., 83.2478, 83.0006, 82.4287],\n",
       "        [80.0940, 78.5115, 76.5741,  ..., 81.8465, 82.3200, 77.8372],\n",
       "        ...,\n",
       "        [75.2231, 81.9015, 80.3835,  ..., 79.8307, 79.5554, 83.6244],\n",
       "        [77.8370, 83.3969, 80.9174,  ..., 82.8269, 82.7837, 82.1167],\n",
       "        [71.1475, 79.2757, 82.5588,  ..., 78.2288, 77.2570, 79.6065]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1622': ['92', '195', '125', '135', '233', '221', '17', '133', '234', '58', '19', '162', '112', '124', '142', '157', '197', '215', '201', '96'], '63': ['101', '40', '160', '71', '92', '189', '221', '168', '125', '17', '58', '231', '195', '229', '120', '152', '233', '62', '133', '50'], '697': ['122', '47', '19', '201', '53', '172', '195', '83', '133', '82', '135', '234', '162', '125', '197', '96', '33', '86', '233', '183'], '566': ['120', '152', '223', '177', '100', '35', '205', '50', '103', '199', '73', '109', '92', '149', '42', '34', '43', '40', '76', '185'], '1868': ['135', '195', '92', '125', '112', '221', '58', '233', '17', '234', '162', '24', '96', '133', '183', '101', '142', '44', '124', '215'], '1716': ['162', '135', '105', '86', '198', '41', '172', '44', '125', '134', '70', '126', '13', '83', '19', '178', '77', '82', '209', '54'], '2263': ['151', '210', '155', '189', '108', '231', '74', '236', '141', '140', '216', '27', '20', '65', '64', '87', '190', '164', '208', '84'], '220': ['162', '41', '135', '134', '157', '54', '77', '178', '32', '125', '189', '82', '44', '198', '209', '126', '144', '164', '85', '65'], '1070': ['205', '181', '35', '89', '107', '199', '149', '34', '92', '204', '109', '76', '99', '118', '153', '120', '15', '152', '228', '17'], '75': ['149', '35', '205', '30', '225', '127', '123', '92', '21', '199', '76', '228', '118', '191', '201', '221', '233', '195', '34', '17'], '554': ['205', '35', '89', '181', '107', '149', '92', '199', '76', '118', '34', '109', '99', '204', '17', '176', '30', '191', '228', '120'], '1282': ['165', '109', '80', '156', '196', '31', '117', '120', '106', '152', '43', '93', '168', '223', '26', '185', '121', '100', '213', '218'], '67': ['156', '80', '93', '111', '22', '165', '173', '163', '43', '61', '117', '185', '213', '26', '94', '196', '121', '72', '23', '211'], '504': ['92', '120', '152', '40', '223', '177', '100', '17', '35', '101', '71', '160', '205', '50', '168', '103', '109', '221', '199', '73'], '1648': ['92', '195', '125', '221', '58', '17', '233', '234', '135', '215', '133', '142', '112', '150', '124', '24', '197', '186', '96', '162'], '1382': ['205', '35', '149', '34', '199', '30', '92', '99', '76', '225', '228', '118', '17', '21', '120', '89', '127', '233', '221', '152'], '559': ['165', '80', '109', '196', '156', '120', '31', '93', '152', '43', '185', '153', '106', '94', '121', '111', '15', '117', '61', '223'], '1494': ['80', '156', '93', '165', '117', '43', '61', '111', '94', '213', '185', '26', '121', '98', '211', '23', '72', '103', '196', '214'], '2053': ['91', '93', '72', '80', '156', '163', '98', '214', '22', '141', '165', '108', '196', '158', '61', '117', '213', '227', '74', '185'], '16': ['151', '210', '189', '231', '155', '236', '108', '74', '87', '65', '140', '190', '141', '216', '27', '20', '164', '64', '208', '219'], '2044': ['80', '156', '117', '93', '165', '43', '94', '185', '26', '214', '168', '121', '111', '213', '211', '23', '61', '98', '91', '103'], '1771': ['41', '135', '134', '77', '162', '157', '54', '126', '32', '178', '209', '82', '125', '229', '189', '198', '44', '231', '65', '190'], '863': ['41', '162', '135', '134', '77', '157', '178', '54', '82', '32', '126', '198', '125', '209', '44', '85', '52', '144', '189', '229'], '394': ['89', '104', '35', '205', '181', '34', '149', '92', '107', '204', '199', '165', '76', '109', '118', '196', '99', '153', '80', '15'], '2428': ['47', '19', '53', '122', '201', '92', '233', '195', '221', '133', '17', '234', '125', '112', '58', '124', '149', '197', '30', '135'], '853': ['205', '35', '92', '149', '199', '34', '76', '17', '118', '99', '228', '120', '30', '225', '221', '191', '152', '21', '123', '58'], '235': ['162', '172', '83', '135', '82', '19', '105', '86', '198', '44', '41', '134', '125', '13', '70', '132', '126', '39', '77', '178'], '245': ['120', '152', '223', '177', '100', '35', '50', '205', '103', '73', '199', '109', '92', '149', '42', '40', '34', '76', '43', '111'], '2150': ['80', '156', '117', '93', '165', '43', '61', '213', '185', '121', '26', '94', '111', '168', '23', '214', '98', '211', '169', '109'], '1531': ['157', '209', '54', '41', '171', '135', '77', '134', '170', '85', '144', '32', '162', '178', '55', '25', '125', '151', '229', '210'], '2047': ['41', '162', '134', '135', '126', '77', '157', '178', '82', '54', '32', '209', '198', '125', '44', '229', '85', '144', '52', '189'], '1851': ['135', '162', '125', '198', '44', '41', '101', '19', '134', '54', '189', '183', '77', '157', '209', '105', '195', '96', '178', '112'], '1360': ['162', '157', '41', '135', '54', '134', '77', '32', '125', '178', '82', '44', '198', '19', '209', '126', '189', '229', '101', '52'], '2468': ['151', '168', '74', '141', '210', '189', '140', '236', '169', '108', '36', '231', '155', '88', '79', '64', '216', '222', '27', '28'], '960': ['162', '41', '134', '135', '126', '178', '77', '54', '209', '157', '85', '198', '32', '82', '144', '125', '44', '229', '212', '105'], '1400': ['171', '209', '170', '157', '151', '41', '144', '54', '77', '210', '55', '189', '135', '25', '134', '85', '74', '141', '32', '108'], '2104': ['157', '54', '92', '90', '32', '133', '233', '195', '221', '125', '174', '17', '135', '41', '77', '193', '234', '19', '112', '162'], '1569': ['135', '162', '125', '198', '44', '19', '41', '101', '134', '157', '195', '77', '96', '54', '32', '82', '183', '105', '178', '234'], '2186': ['165', '80', '196', '109', '156', '106', '93', '31', '152', '120', '43', '153', '94', '15', '218', '61', '117', '111', '185', '121'], '1464': ['92', '35', '205', '149', '76', '191', '118', '58', '195', '127', '199', '221', '17', '123', '225', '30', '99', '21', '233', '215'], '1787': ['144', '209', '85', '41', '55', '162', '134', '135', '170', '171', '210', '77', '54', '178', '157', '151', '126', '20', '155', '25'], '116': ['80', '93', '156', '117', '165', '43', '61', '91', '213', '98', '185', '214', '94', '72', '169', '121', '26', '111', '163', '196'], '578': ['41', '135', '157', '162', '77', '134', '54', '32', '125', '178', '82', '209', '44', '198', '189', '126', '101', '19', '229', '171'], '734': ['35', '205', '149', '199', '34', '92', '76', '99', '118', '120', '152', '228', '30', '177', '17', '89', '181', '225', '191', '21'], '202': ['111', '43', '185', '103', '26', '173', '94', '211', '23', '121', '15', '73', '156', '109', '152', '168', '153', '80', '93', '163'], '590': ['209', '157', '41', '162', '134', '85', '54', '144', '135', '77', '178', '171', '170', '55', '32', '125', '25', '82', '44', '126'], '256': ['195', '125', '135', '234', '92', '233', '19', '133', '221', '162', '58', '142', '17', '96', '124', '186', '112', '201', '197', '24'], '705': ['84', '91', '214', '155', '108', '74', '115', '141', '236', '38', '151', '36', '210', '140', '98', '231', '64', '93', '27', '137'], '1612': ['135', '162', '125', '195', '112', '19', '183', '96', '133', '233', '234', '124', '198', '92', '221', '44', '24', '201', '101', '142'], '925': ['162', '135', '41', '134', '77', '178', '54', '157', '126', '209', '198', '32', '82', '125', '44', '85', '144', '189', '105', '52'], '939': ['125', '135', '195', '162', '92', '234', '233', '19', '221', '133', '17', '58', '44', '124', '157', '112', '142', '96', '215', '197'], '531': ['135', '162', '125', '19', '44', '157', '198', '101', '195', '54', '92', '233', '41', '112', '32', '133', '221', '234', '17', '77'], '677': ['135', '195', '125', '162', '234', '92', '233', '133', '221', '142', '124', '19', '112', '58', '96', '183', '17', '24', '197', '44'], '2376': ['157', '54', '92', '125', '41', '32', '77', '195', '221', '90', '189', '174', '135', '17', '133', '58', '143', '142', '171', '233'], '1644': ['135', '162', '195', '125', '183', '112', '96', '234', '24', '44', '133', '233', '198', '101', '124', '142', '92', '221', '19', '41'], '1545': ['19', '53', '122', '47', '195', '135', '125', '233', '92', '201', '133', '234', '221', '162', '17', '112', '124', '157', '58', '96'], '434': ['35', '205', '149', '199', '76', '92', '34', '99', '118', '120', '152', '191', '228', '177', '30', '17', '223', '89', '225', '181'], '1849': ['162', '135', '125', '86', '198', '44', '105', '41', '19', '134', '157', '82', '77', '70', '101', '178', '32', '172', '54', '83'], '414': ['41', '162', '134', '135', '77', '178', '157', '54', '126', '189', '209', '32', '198', '144', '125', '82', '44', '65', '85', '164'], '962': ['92', '157', '171', '54', '170', '17', '104', '221', '193', '58', '200', '209', '90', '143', '233', '151', '32', '150', '195', '125'], '2510': ['156', '168', '214', '80', '117', '91', '93', '151', '165', '141', '38', '74', '43', '98', '213', '189', '36', '94', '138', '26'], '1623': ['165', '109', '196', '80', '152', '156', '106', '120', '117', '168', '31', '218', '43', '223', '94', '93', '214', '26', '50', '100'], '1412': ['157', '92', '54', '125', '32', '195', '221', '17', '77', '90', '41', '58', '174', '189', '143', '233', '142', '133', '150', '193'], '1736': ['168', '189', '231', '219', '43', '185', '224', '121', '87', '194', '169', '164', '94', '151', '102', '26', '141', '23', '40', '74'], '2138': ['205', '181', '35', '89', '107', '92', '199', '149', '204', '76', '109', '118', '34', '99', '176', '17', '191', '104', '120', '221'], '1968': ['157', '54', '209', '171', '41', '135', '174', '77', '90', '92', '170', '32', '133', '162', '125', '195', '221', '134', '112', '233'], '581': ['151', '189', '74', '210', '141', '140', '231', '236', '168', '108', '155', '36', '64', '216', '219', '27', '79', '164', '87', '222'], '81': ['35', '205', '149', '92', '76', '195', '118', '30', '191', '127', '225', '58', '123', '221', '199', '17', '234', '21', '228', '99'], '800': ['205', '35', '149', '92', '199', '34', '76', '30', '118', '228', '225', '99', '17', '120', '21', '221', '191', '127', '123', '233'], '679': ['35', '205', '149', '199', '34', '92', '76', '99', '118', '120', '152', '228', '30', '177', '17', '89', '181', '225', '191', '21'], '535': ['92', '17', '221', '233', '58', '150', '195', '125', '186', '142', '133', '215', '234', '112', '35', '135', '205', '149', '157', '19'], '2392': ['157', '162', '41', '54', '82', '32', '134', '135', '77', '178', '19', '125', '44', '198', '126', '52', '229', '209', '90', '174'], '2223': ['93', '156', '163', '80', '22', '98', '91', '214', '72', '165', '227', '117', '61', '158', '213', '196', '111', '185', '59', '43'], '798': ['205', '35', '181', '89', '92', '149', '199', '107', '34', '109', '204', '76', '99', '118', '17', '43', '120', '228', '30', '153'], '1662': ['162', '135', '19', '198', '125', '82', '44', '172', '41', '105', '101', '134', '53', '86', '54', '83', '157', '122', '77', '183'], '1003': ['151', '189', '231', '210', '236', '74', '141', '87', '140', '190', '108', '65', '155', '164', '36', '219', '216', '168', '27', '64'], '2320': ['171', '170', '157', '55', '91', '144', '209', '77', '108', '41', '54', '85', '134', '25', '141', '151', '231', '178', '210', '236'], '1164': ['156', '80', '93', '117', '91', '165', '214', '168', '213', '43', '185', '98', '61', '26', '23', '121', '72', '111', '94', '103'], '672': ['162', '135', '198', '125', '44', '19', '41', '134', '101', '157', '77', '82', '86', '54', '105', '178', '32', '96', '183', '195'], '625': ['205', '89', '35', '199', '181', '34', '204', '149', '92', '107', '153', '99', '15', '109', '76', '152', '118', '120', '177', '104'], '1424': ['189', '65', '231', '190', '87', '164', '102', '219', '210', '151', '229', '20', '108', '155', '216', '208', '27', '236', '101', '140'], '666': ['35', '205', '149', '92', '30', '76', '225', '199', '127', '118', '123', '221', '21', '228', '17', '191', '195', '233', '34', '58'], '1483': ['92', '201', '233', '195', '221', '133', '17', '125', '234', '19', '47', '112', '58', '53', '124', '135', '122', '197', '215', '149'], '2277': ['120', '152', '223', '177', '100', '35', '205', '50', '103', '199', '73', '109', '92', '149', '42', '34', '43', '40', '76', '185'], '1013': ['109', '120', '152', '43', '111', '223', '26', '185', '153', '165', '121', '94', '100', '218', '15', '23', '211', '177', '50', '168'], '556': ['135', '195', '125', '162', '234', '19', '233', '133', '124', '92', '142', '96', '221', '183', '58', '112', '17', '24', '197', '44'], '2530': ['41', '134', '162', '135', '77', '178', '157', '144', '209', '189', '54', '126', '85', '32', '125', '198', '65', '44', '164', '82'], '1292': ['157', '54', '92', '195', '125', '32', '90', '41', '221', '77', '174', '133', '17', '233', '58', '135', '234', '193', '197', '112'], '230': ['43', '111', '94', '121', '185', '26', '109', '211', '23', '173', '120', '80', '168', '103', '152', '73', '153', '156', '15', '177'], '119': ['120', '177', '103', '152', '73', '223', '100', '173', '50', '111', '43', '94', '121', '26', '35', '185', '205', '211', '109', '23'], '278': ['151', '189', '210', '74', '231', '141', '140', '155', '236', '108', '216', '164', '27', '64', '87', '219', '20', '208', '65', '190'], '1591': ['109', '120', '43', '168', '152', '50', '223', '121', '218', '100', '26', '94', '23', '185', '40', '111', '165', '211', '153', '62'], '2009': ['151', '25', '210', '189', '155', '216', '27', '101', '140', '20', '229', '108', '231', '164', '74', '64', '141', '208', '65', '102'], '1071': ['195', '135', '92', '125', '233', '221', '234', '17', '133', '124', '58', '19', '162', '142', '112', '157', '96', '197', '215', '186'], '161': ['43', '111', '185', '26', '168', '23', '121', '103', '94', '211', '156', '80', '93', '109', '73', '165', '173', '117', '15', '152'], '968': ['189', '101', '231', '219', '164', '168', '229', '87', '151', '190', '65', '62', '102', '210', '25', '224', '40', '160', '71', '157'], '2527': ['91', '156', '93', '214', '117', '80', '168', '98', '165', '213', '43', '61', '138', '59', '185', '74', '141', '26', '38', '151'], '1342': ['162', '135', '198', '125', '44', '41', '134', '19', '105', '77', '82', '126', '178', '86', '101', '157', '54', '32', '209', '172'], '2463': ['92', '35', '149', '205', '76', '191', '195', '118', '58', '127', '221', '17', '123', '225', '30', '199', '21', '125', '233', '234'], '2183': ['109', '120', '152', '43', '223', '165', '100', '218', '26', '185', '153', '121', '15', '50', '111', '94', '168', '23', '211', '177'], '29': ['151', '189', '210', '74', '231', '141', '140', '236', '155', '164', '219', '216', '27', '108', '87', '190', '64', '36', '65', '168'], '1828': ['35', '205', '149', '199', '92', '34', '76', '99', '118', '30', '228', '120', '17', '225', '21', '221', '191', '152', '123', '177'], '1729': ['189', '65', '229', '190', '231', '87', '164', '102', '101', '41', '219', '210', '134', '162', '151', '135', '25', '77', '20', '157'], '1362': ['189', '164', '231', '65', '190', '219', '87', '151', '102', '210', '155', '74', '216', '140', '101', '27', '208', '236', '141', '20'], '676': ['205', '35', '149', '92', '76', '199', '118', '34', '99', '30', '191', '228', '17', '225', '120', '58', '221', '127', '21', '123'], '1626': ['43', '185', '111', '94', '103', '26', '121', '211', '15', '23', '173', '93', '168', '80', '156', '153', '109', '73', '152', '91'], '283': ['43', '111', '94', '121', '185', '173', '26', '211', '109', '23', '80', '120', '153', '103', '168', '15', '152', '73', '156', '177'], '1102': ['135', '162', '125', '44', '41', '198', '86', '157', '105', '134', '19', '101', '77', '54', '32', '178', '195', '133', '82', '112'], '1151': ['195', '125', '135', '234', '92', '233', '19', '133', '221', '162', '58', '142', '17', '96', '124', '186', '112', '201', '197', '24'], '2419': ['135', '162', '195', '183', '125', '96', '19', '234', '112', '198', '124', '24', '44', '133', '233', '142', '201', '41', '101', '92'], '2222': ['135', '162', '133', '195', '125', '19', '112', '122', '233', '157', '234', '96', '92', '44', '221', '41', '201', '86', '54', '197'], '668': ['109', '15', '153', '120', '152', '43', '177', '111', '173', '185', '121', '94', '218', '223', '26', '165', '100', '205', '35', '23'], '1269': ['157', '162', '41', '135', '54', '134', '77', '32', '178', '125', '82', '44', '198', '209', '19', '126', '189', '229', '101', '52'], '453': ['205', '35', '149', '199', '92', '34', '76', '99', '118', '120', '152', '30', '228', '177', '17', '89', '225', '181', '191', '21'], '2315': ['205', '35', '149', '199', '92', '34', '76', '99', '118', '120', '30', '228', '152', '17', '225', '89', '177', '181', '191', '21'], '2208': ['195', '149', '35', '30', '205', '127', '92', '76', '225', '234', '118', '191', '125', '233', '58', '21', '123', '221', '228', '17'], '757': ['157', '54', '41', '32', '135', '195', '125', '77', '162', '234', '90', '133', '134', '57', '19', '174', '82', '233', '96', '142'], '1621': ['195', '92', '125', '234', '233', '135', '221', '58', '17', '133', '142', '19', '124', '112', '215', '197', '162', '186', '24', '96'], '1544': ['189', '231', '164', '65', '190', '219', '87', '151', '102', '210', '155', '74', '216', '101', '140', '27', '236', '208', '229', '141'], '387': ['156', '93', '80', '91', '98', '61', '165', '213', '117', '214', '72', '185', '43', '163', '111', '158', '94', '26', '106', '22'], '1934': ['151', '210', '189', '74', '155', '141', '140', '236', '216', '231', '27', '108', '164', '64', '36', '208', '20', '65', '190', '219'], '1368': ['80', '156', '93', '43', '185', '165', '111', '61', '117', '94', '121', '213', '26', '72', '23', '163', '98', '196', '22', '109'], '1975': ['65', '189', '190', '229', '231', '164', '87', '102', '101', '210', '41', '219', '162', '134', '135', '151', '25', '20', '126', '155'], '1345': ['120', '152', '223', '100', '177', '50', '35', '103', '205', '73', '109', '92', '149', '40', '42', '199', '43', '76', '185', '121'], '1': ['151', '189', '210', '74', '141', '140', '236', '155', '231', '108', '36', '216', '27', '64', '164', '219', '168', '87', '20', '208'], '822': ['101', '40', '92', '160', '71', '120', '195', '133', '233', '221', '17', '125', '152', '19', '234', '58', '189', '50', '135', '177'], '49': ['41', '162', '134', '135', '126', '178', '77', '157', '54', '209', '82', '198', '32', '85', '144', '44', '125', '229', '52', '105'], '2357': ['189', '231', '164', '219', '87', '65', '190', '151', '102', '210', '155', '74', '141', '108', '140', '101', '236', '216', '27', '64'], '2229': ['92', '17', '221', '233', '58', '125', '195', '35', '205', '149', '215', '133', '150', '186', '234', '112', '135', '157', '19', '199'], '2109': ['195', '92', '135', '125', '233', '19', '234', '221', '133', '17', '162', '201', '112', '124', '58', '142', '157', '186', '96', '53'], '780': ['189', '231', '164', '65', '219', '151', '190', '87', '210', '102', '168', '74', '155', '166', '141', '236', '140', '159', '216', '224'], '146': ['120', '152', '177', '223', '50', '35', '100', '205', '149', '40', '42', '109', '101', '92', '30', '68', '71', '160', '73', '199'], '2227': ['205', '35', '149', '199', '34', '92', '76', '99', '118', '120', '30', '228', '152', '177', '225', '17', '181', '89', '21', '191'], '2402': ['157', '92', '54', '195', '32', '125', '221', '90', '17', '58', '77', '41', '174', '193', '133', '233', '234', '135', '215', '197'], '217': ['41', '135', '134', '162', '77', '157', '54', '178', '209', '32', '125', '126', '82', '189', '198', '44', '229', '101', '85', '52'], '1232': ['93', '156', '80', '91', '117', '214', '98', '165', '43', '168', '213', '185', '61', '94', '26', '121', '111', '211', '23', '103'], '175': ['111', '43', '185', '103', '26', '173', '211', '94', '23', '121', '15', '156', '73', '109', '168', '152', '80', '153', '93', '163'], '1929': ['165', '109', '80', '156', '196', '31', '120', '117', '106', '152', '93', '43', '223', '185', '213', '121', '26', '100', '153', '218'], '1942': ['189', '231', '87', '190', '65', '164', '219', '168', '151', '102', '210', '166', '236', '229', '224', '74', '141', '140', '108', '159'], '325': ['109', '43', '120', '168', '152', '165', '111', '26', '223', '185', '121', '218', '15', '94', '23', '153', '100', '211', '50', '177'], '1205': ['135', '162', '105', '41', '198', '86', '125', '44', '19', '172', '134', '101', '77', '126', '70', '82', '54', '189', '209', '157'], '1394': ['135', '41', '162', '134', '157', '54', '77', '189', '32', '209', '125', '178', '82', '126', '198', '44', '19', '231', '164', '65'], '2213': ['80', '156', '117', '93', '165', '43', '94', '185', '26', '214', '111', '121', '213', '168', '211', '23', '61', '98', '91', '103'], '1073': ['209', '171', '41', '157', '170', '77', '135', '54', '134', '144', '151', '85', '25', '174', '57', '32', '200', '55', '162', '125'], '537': ['151', '210', '189', '155', '74', '216', '27', '140', '108', '231', '141', '20', '208', '164', '236', '64', '65', '190', '87', '219'], '510': ['189', '231', '168', '219', '164', '87', '151', '190', '65', '102', '224', '210', '194', '101', '74', '141', '159', '166', '140', '236'], '790': ['91', '214', '141', '74', '151', '156', '93', '80', '117', '108', '168', '98', '236', '36', '38', '140', '155', '165', '189', '210'], '2524': ['168', '189', '231', '219', '164', '87', '151', '102', '190', '224', '65', '210', '194', '236', '74', '141', '101', '14', '140', '159'], '1111': ['35', '149', '205', '92', '76', '118', '199', '127', '195', '191', '99', '30', '225', '34', '58', '221', '17', '123', '120', '233'], '1313': ['41', '135', '162', '134', '157', '77', '189', '54', '178', '209', '32', '125', '231', '164', '126', '65', '198', '44', '82', '190'], '306': ['205', '35', '89', '181', '149', '92', '107', '199', '76', '109', '30', '118', '34', '225', '17', '127', '228', '191', '99', '221'], '570': ['91', '214', '84', '115', '38', '93', '98', '156', '59', '117', '80', '108', '72', '163', '165', '138', '236', '227', '168', '61'], '857': ['41', '135', '162', '157', '134', '77', '54', '32', '125', '178', '82', '57', '209', '198', '174', '126', '44', '195', '96', '90'], '1492': ['157', '54', '174', '41', '135', '92', '77', '90', '32', '133', '125', '195', '233', '221', '162', '112', '17', '134', '57', '193'], '583': ['195', '125', '135', '92', '221', '58', '234', '233', '17', '162', '133', '142', '124', '112', '215', '96', '183', '24', '19', '150'], '558': ['151', '210', '189', '155', '216', '74', '27', '140', '108', '231', '20', '208', '141', '64', '164', '236', '65', '102', '190', '87'], '180': ['135', '41', '162', '134', '77', '54', '189', '157', '209', '178', '126', '125', '32', '231', '164', '198', '65', '44', '190', '144'], '1601': ['156', '93', '80', '91', '98', '61', '165', '117', '213', '214', '72', '163', '43', '111', '185', '158', '106', '22', '196', '26'], '580': ['151', '210', '189', '74', '155', '141', '140', '236', '108', '216', '231', '27', '64', '36', '164', '20', '208', '219', '190', '65'], '2297': ['151', '210', '189', '155', '74', '231', '216', '140', '27', '141', '108', '208', '164', '236', '20', '65', '64', '190', '87', '219'], '2180': ['151', '25', '210', '189', '155', '216', '140', '101', '27', '20', '231', '229', '108', '102', '236', '74', '164', '141', '208', '64'], '179': ['35', '205', '199', '34', '92', '149', '76', '99', '118', '152', '120', '177', '223', '191', '89', '17', '100', '228', '181', '107'], '603': ['43', '185', '121', '111', '94', '26', '23', '168', '211', '109', '103', '80', '120', '73', '156', '152', '173', '95', '223', '93'], '1697': ['19', '122', '53', '133', '47', '195', '135', '125', '233', '162', '201', '157', '234', '92', '221', '197', '112', '17', '82', '54'], '543': ['135', '195', '125', '162', '234', '19', '233', '133', '124', '92', '142', '96', '221', '183', '58', '112', '17', '24', '197', '44'], '335': ['35', '205', '92', '149', '76', '199', '118', '191', '99', '17', '58', '221', '34', '225', '127', '30', '120', '123', '228', '195'], '2373': ['41', '135', '157', '162', '77', '134', '54', '189', '32', '125', '178', '209', '231', '44', '164', '82', '171', '198', '65', '190'], '1448': ['92', '195', '233', '17', '125', '221', '234', '58', '186', '19', '135', '133', '215', '149', '205', '35', '142', '124', '112', '30'], '1199': ['189', '231', '87', '65', '190', '164', '102', '219', '151', '210', '236', '229', '74', '155', '108', '101', '140', '141', '216', '20'], '1990': ['135', '195', '92', '125', '162', '233', '221', '19', '234', '17', '133', '112', '124', '58', '142', '201', '96', '157', '183', '215'], '2048': ['151', '210', '189', '231', '108', '74', '155', '141', '140', '236', '20', '216', '27', '164', '65', '64', '87', '190', '208', '219'], '483': ['80', '156', '43', '94', '93', '121', '185', '61', '111', '117', '165', '213', '26', '211', '109', '23', '72', '98', '196', '169'], '1093': ['162', '135', '105', '198', '86', '125', '44', '41', '19', '70', '134', '172', '82', '77', '101', '54', '157', '178', '32', '83'], '1074': ['80', '93', '156', '43', '94', '165', '185', '117', '72', '163', '196', '98', '213', '121', '61', '214', '26', '22', '111', '158'], '1184': ['189', '231', '65', '190', '87', '164', '151', '219', '210', '102', '236', '229', '155', '74', '108', '140', '216', '20', '27', '141'], '1708': ['171', '170', '157', '209', '151', '54', '77', '41', '144', '92', '55', '189', '210', '174', '25', '74', '32', '141', '200', '135'], '2496': ['92', '157', '17', '221', '32', '54', '193', '58', '233', '195', '90', '125', '174', '89', '181', '77', '186', '35', '133', '205'], '249': ['19', '135', '195', '53', '122', '125', '162', '47', '234', '201', '233', '96', '133', '124', '82', '142', '183', '92', '221', '112'], '1021': ['133', '195', '135', '92', '125', '233', '19', '234', '221', '162', '197', '17', '122', '157', '112', '201', '124', '58', '142', '96'], '860': ['35', '205', '92', '149', '199', '76', '118', '34', '120', '99', '17', '152', '221', '191', '225', '228', '30', '123', '127', '21'], '630': ['195', '125', '234', '92', '135', '233', '19', '221', '58', '17', '133', '142', '162', '186', '124', '96', '112', '197', '201', '215'], '482': ['195', '30', '133', '149', '35', '92', '127', '205', '225', '221', '135', '233', '234', '123', '125', '197', '21', '17', '112', '58'], '1138': ['151', '210', '189', '155', '74', '108', '216', '141', '27', '140', '236', '231', '64', '20', '208', '164', '65', '190', '219', '87'], '706': ['205', '35', '149', '92', '76', '199', '118', '99', '30', '191', '34', '225', '228', '127', '17', '58', '120', '221', '195', '21'], '633': ['162', '125', '135', '19', '233', '195', '92', '157', '133', '221', '17', '234', '112', '53', '44', '54', '124', '201', '122', '198'], '2342': ['92', '157', '89', '181', '17', '35', '205', '221', '149', '54', '193', '107', '233', '90', '199', '58', '174', '32', '109', '34'], '1124': ['135', '162', '41', '125', '105', '198', '86', '44', '134', '77', '172', '157', '101', '178', '70', '19', '209', '126', '54', '82'], '2185': ['132', '212', '172', '162', '39', '105', '135', '83', '86', '13', '82', '70', '41', '134', '198', '19', '126', '44', '125', '178'], '1396': ['92', '195', '125', '233', '135', '221', '17', '133', '234', '19', '58', '162', '112', '124', '142', '157', '201', '186', '197', '215'], '128': ['205', '35', '149', '92', '199', '34', '76', '30', '118', '228', '225', '99', '17', '120', '21', '221', '191', '127', '123', '233'], '2535': ['84', '210', '20', '155', '151', '108', '216', '27', '65', '189', '208', '64', '74', '236', '190', '140', '231', '141', '164', '87'], '2121': ['162', '135', '198', '86', '125', '105', '44', '41', '134', '19', '70', '82', '77', '172', '157', '178', '101', '32', '54', '83'], '2458': ['135', '41', '162', '134', '54', '77', '157', '189', '209', '178', '32', '126', '125', '198', '44', '82', '231', '164', '65', '144'], '2350': ['157', '171', '92', '209', '54', '170', '200', '32', '90', '221', '77', '125', '17', '195', '41', '193', '174', '58', '133', '233'], '2153': ['189', '231', '229', '87', '101', '190', '65', '164', '219', '168', '102', '151', '157', '62', '210', '25', '41', '166', '236', '224'], '421': ['109', '120', '111', '43', '152', '153', '15', '218', '94', '121', '223', '26', '165', '185', '211', '100', '177', '23', '35', '205'], '542': ['101', '135', '189', '40', '71', '160', '162', '14', '195', '125', '183', '231', '164', '219', '96', '92', '112', '234', '229', '19'], '1017': ['132', '212', '39', '172', '162', '105', '83', '135', '13', '86', '70', '134', '41', '198', '126', '82', '44', '178', '19', '85'], '148': ['205', '35', '149', '199', '92', '34', '76', '99', '118', '120', '30', '228', '152', '17', '177', '191', '225', '89', '181', '127'], '961': ['93', '80', '156', '72', '163', '22', '61', '91', '158', '98', '213', '165', '196', '117', '185', '111', '43', '214', '31', '206'], '2499': ['189', '231', '65', '190', '164', '87', '101', '219', '151', '102', '210', '155', '229', '236', '216', '135', '74', '27', '140', '20'], '1989': ['80', '156', '117', '93', '165', '43', '185', '111', '26', '61', '213', '121', '168', '94', '23', '211', '214', '98', '109', '196'], '1154': ['151', '189', '210', '231', '74', '140', '164', '155', '141', '236', '216', '65', '27', '190', '208', '219', '87', '108', '20', '64'], '2525': ['109', '15', '153', '152', '120', '185', '43', '177', '121', '223', '94', '100', '165', '218', '173', '111', '26', '23', '196', '50'], '1409': ['209', '144', '170', '171', '55', '41', '85', '157', '134', '77', '151', '210', '54', '25', '162', '178', '135', '155', '108', '189'], '1822': ['151', '210', '189', '74', '236', '141', '140', '155', '231', '108', '36', '216', '27', '64', '164', '219', '168', '87', '20', '208'], '392': ['189', '101', '231', '164', '219', '229', '168', '87', '190', '151', '65', '102', '210', '25', '62', '224', '40', '160', '71', '157'], '752': ['65', '189', '190', '231', '164', '87', '210', '102', '155', '151', '219', '20', '216', '108', '27', '208', '229', '64', '74', '236'], '2457': ['135', '162', '41', '105', '125', '198', '86', '44', '134', '77', '172', '70', '19', '157', '178', '101', '82', '126', '83', '54'], '1469': ['41', '135', '134', '162', '77', '157', '189', '54', '126', '178', '231', '65', '209', '32', '190', '229', '87', '125', '82', '144'], '1375': ['135', '162', '125', '44', '19', '198', '101', '41', '157', '195', '54', '77', '92', '183', '112', '134', '96', '32', '234', '233'], '1897': ['162', '135', '41', '134', '77', '157', '54', '178', '126', '209', '32', '125', '82', '198', '44', '85', '144', '19', '189', '52'], '145': ['151', '210', '189', '231', '155', '236', '74', '141', '108', '140', '216', '27', '36', '190', '65', '87', '64', '20', '164', '208'], '1482': ['151', '155', '210', '189', '74', '216', '27', '108', '140', '141', '64', '236', '20', '208', '231', '164', '65', '190', '87', '219'], '1275': ['162', '19', '82', '157', '135', '54', '41', '134', '77', '125', '32', '44', '178', '198', '101', '53', '122', '209', '189', '229'], '2163': ['41', '157', '135', '77', '54', '32', '134', '174', '125', '57', '195', '162', '90', '178', '133', '112', '92', '96', '139', '209'], '399': ['156', '80', '93', '165', '117', '61', '43', '213', '111', '98', '185', '163', '72', '26', '214', '196', '22', '91', '94', '158'], '2437': ['189', '231', '168', '219', '164', '87', '151', '190', '65', '102', '224', '210', '74', '194', '101', '141', '159', '166', '140', '236'], '203': ['172', '162', '83', '19', '82', '135', '86', '105', '198', '122', '44', '13', '70', '125', '47', '41', '132', '134', '53', '39'], '471': ['155', '108', '151', '210', '74', '27', '216', '20', '141', '140', '64', '236', '189', '231', '208', '65', '84', '164', '190', '25'], '1509': ['189', '101', '14', '231', '164', '219', '168', '151', '87', '40', '160', '71', '102', '190', '65', '210', '88', '224', '68', '159'], '1713': ['41', '162', '135', '134', '77', '82', '178', '157', '32', '126', '198', '54', '125', '44', '209', '52', '85', '19', '189', '105'], '596': ['47', '19', '53', '122', '201', '195', '233', '92', '133', '221', '234', '17', '125', '112', '30', '124', '58', '135', '149', '197'], '304': ['189', '168', '231', '219', '164', '87', '151', '190', '224', '102', '65', '210', '74', '141', '194', '101', '236', '140', '166', '108'], '2258': ['80', '156', '93', '43', '185', '165', '117', '61', '111', '121', '94', '213', '26', '23', '72', '98', '109', '196', '211', '163'], '624': ['196', '165', '15', '22', '31', '163', '153', '206', '80', '106', '93', '156', '109', '152', '173', '72', '177', '120', '61', '111'], '1508': ['80', '156', '117', '93', '165', '43', '214', '94', '168', '185', '26', '213', '121', '91', '23', '61', '211', '98', '111', '138'], '1571': ['120', '152', '223', '177', '100', '35', '205', '50', '103', '73', '109', '199', '92', '149', '42', '43', '34', '40', '76', '185'], '1063': ['151', '189', '210', '231', '236', '87', '108', '74', '155', '65', '140', '190', '141', '164', '216', '20', '27', '64', '208', '219'], '1480': ['189', '101', '231', '229', '164', '87', '219', '168', '190', '65', '151', '102', '135', '62', '210', '25', '41', '157', '40', '71'], '1326': ['157', '171', '170', '92', '54', '209', '151', '55', '141', '108', '74', '144', '104', '236', '77', '91', '210', '25', '140', '41'], '2371': ['162', '135', '105', '86', '198', '172', '41', '44', '70', '134', '125', '19', '13', '126', '83', '54', '178', '157', '82', '77'], '518': ['35', '205', '92', '149', '76', '199', '118', '191', '17', '58', '99', '225', '221', '30', '127', '34', '228', '123', '120', '195'], '1816': ['120', '152', '177', '35', '223', '205', '50', '100', '149', '109', '92', '199', '73', '103', '76', '34', '40', '42', '118', '43'], '605': ['101', '135', '189', '162', '65', '229', '231', '190', '164', '87', '41', '102', '105', '125', '86', '219', '210', '151', '198', '25'], '1004': ['173', '34', '22', '35', '205', '153', '89', '94', '204', '15', '111', '181', '199', '163', '43', '185', '109', '121', '177', '211'], '1704': ['91', '214', '80', '93', '156', '117', '98', '38', '61', '165', '213', '59', '72', '168', '43', '185', '169', '163', '138', '94'], '761': ['35', '205', '149', '92', '199', '76', '17', '30', '225', '221', '118', '191', '228', '34', '123', '127', '120', '21', '99', '58'], '802': ['35', '205', '149', '92', '76', '195', '118', '30', '191', '127', '225', '58', '123', '221', '199', '17', '234', '21', '228', '99'], '1907': ['41', '135', '162', '157', '77', '134', '54', '32', '125', '82', '178', '57', '198', '209', '174', '44', '126', '195', '96', '90'], '184': ['43', '111', '26', '185', '121', '94', '23', '211', '168', '109', '103', '80', '156', '73', '173', '120', '152', '223', '93', '165'], '0': ['157', '92', '54', '125', '32', '195', '221', '17', '77', '90', '41', '58', '174', '189', '143', '233', '142', '133', '150', '193'], '2': ['80', '156', '117', '43', '165', '93', '111', '185', '26', '121', '61', '94', '213', '168', '23', '211', '109', '98', '214', '196'], '3': ['209', '41', '135', '157', '77', '162', '134', '54', '171', '85', '144', '32', '170', '125', '178', '151', '25', '82', '210', '189'], '4': ['162', '135', '198', '86', '125', '105', '44', '41', '134', '19', '70', '82', '77', '172', '157', '178', '101', '32', '54', '83'], '5': ['132', '212', '39', '172', '162', '135', '105', '83', '13', '86', '70', '134', '41', '198', '126', '82', '19', '44', '178', '125'], '6': ['43', '94', '111', '121', '185', '211', '26', '109', '173', '23', '120', '103', '80', '152', '73', '153', '168', '15', '177', '156'], '7': ['210', '151', '155', '189', '20', '216', '27', '208', '108', '65', '231', '190', '140', '236', '164', '74', '64', '87', '141', '102'], '8': ['120', '177', '152', '103', '73', '223', '173', '100', '43', '50', '111', '94', '121', '35', '205', '185', '109', '26', '199', '211'], '9': ['80', '156', '93', '43', '117', '185', '165', '121', '94', '111', '61', '26', '213', '168', '23', '109', '211', '214', '98', '72'], '10': ['151', '74', '141', '189', '210', '236', '140', '108', '36', '231', '155', '168', '64', '216', '27', '79', '28', '38', '207', '222'], '11': ['101', '40', '71', '160', '189', '92', '14', '135', '125', '195', '221', '152', '231', '58', '168', '17', '219', '112', '164', '120'], '12': ['43', '111', '173', '185', '94', '121', '26', '23', '103', '211', '15', '109', '80', '153', '156', '93', '22', '152', '168', '120']}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_k_similar_users(scores_matrix, query_user_names, corpus_user_names, k):\n",
    "    top_k_similar_users = {}\n",
    "    scores_matrix = scores_matrix.detach().cpu().numpy()\n",
    "\n",
    "    for i, curr_scores in enumerate(scores_matrix):\n",
    "        # Get the indices of the top k scores\n",
    "        top_k_indices = np.argsort(curr_scores)[-k:][::-1]\n",
    "        \n",
    "        # Map indices to corresponding user names in the corpus_user_names list\n",
    "        top_k_users = [corpus_user_names[idx] for idx in top_k_indices]\n",
    "        \n",
    "        # Map the query user name to the list of top k similar users\n",
    "        top_k_similar_users[query_user_names[i]] = top_k_users\n",
    "\n",
    "    return top_k_similar_users\n",
    "\n",
    "k = 20  # Number of top similar users to retrieve\n",
    "result = get_top_k_similar_users(scores, test_user_list, train_user_list, k)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5726141078838174\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ubuntu/code/pump_post_midterm/pump\")\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "# path = \"opinions_qa/review/American_Trends_Panel_W34/date0901_v16_known_test_rag_2000_40_withname_American_Trends_Panel_W34_haikupred_prompt3_history_demo_persona_run1\"\n",
    "# path = \"opinions_qa/review/American_Trends_Panel_W34/date0901_v16_known_test_rag_500_40_withname_American_Trends_Panel_W34_haikupred_prompt3_history_demo_persona_run1\"\n",
    "path = \"opinions_qa/review/American_Trends_Panel_W34/date0901_v16_known_test_rag_train2000_top40_skew10_usedemoTrue_withname_American_Trends_Panel_W34_haikupred_prompt3_history_run1\"\n",
    "\n",
    "# Usage example\n",
    "start_marker = \"# Similar users' answers\"\n",
    "end_marker = \"# Your Prediction of the Participant's Response (answer with given options verbatim, no formatting, no punctuations or linebreaks at the end, don't explain)\"\n",
    "\n",
    "\n",
    "def extract_text_between_markers(file_path, start_marker, end_marker):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    gold = lines[-1][len(\"Gold: \"):]\n",
    "\n",
    "    # Initialize variables to track whether we are within the markers\n",
    "    capturing = False\n",
    "    extracted_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if start_marker in line:\n",
    "            capturing = True\n",
    "            continue  # Skip the start marker line itself\n",
    "        elif end_marker in line:\n",
    "            capturing = False\n",
    "            break  # Stop once we reach the end marker line\n",
    "\n",
    "        if capturing:\n",
    "            extracted_lines.append(line.strip())  # Strip any surrounding whitespace\n",
    "\n",
    "    return gold, \"\\n\".join(extracted_lines)\n",
    "\n",
    "correct, total = 0, 0\n",
    "for filename in os.listdir(path):\n",
    "    file_path = f\"{path}/{filename}\"\n",
    "    gold, extracted_text = extract_text_between_markers(file_path, start_marker, end_marker)\n",
    "    most = Counter(extracted_text.split(\"\\n\")).most_common(1)[0][0]\n",
    "    if most == gold: correct += 1\n",
    "    total += 1\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A big problem\\nA big problem\\nA small problem\\nA small problem\\nA big problem\\nNot a problem\\nNot a problem\\nA big problem\\nA small problem\\nA small problem\\nA small problem\\nA big problem\\nA small problem\\nA big problem\\nA small problem\\nA big problem\\nA big problem\\nA small problem\\nA big problem\\nA big problem\\nA big problem\\nA small problem\\nA small problem\\nA small problem\\nA big problem\\nA big problem\\nA big problem\\nA small problem\\nA small problem\\nA small problem\\nA big problem\\nNot a problem\\nA big problem\\nNot a problem\\nA big problem\\nA small problem\\nA small problem\\nA big problem\\nA small problem\\nA small problem\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from src.utils import get_file_from_s3\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "\n",
    "    with open(args.persona_val_path, 'r') as f:\n",
    "        p_vals = json.load(f)\n",
    "    print(\"Number of users:\", len(list(p_vals.keys())))\n",
    "\n",
    "\n",
    "    resp_df = pd.read_csv(get_file_from_s3(f\"human_resp/{args.survey_name}/responses.csv\"))\n",
    "    test_user_idx = random.choices(range(len(resp_df)), k=int(len(resp_df)*0.1))\n",
    "    test_resp_df = resp_df.iloc[test_user_idx]\n",
    "    test_len = int(len(resp_df)*0.1)\n",
    "\n",
    "    test_user_list = list(p_vals.keys())[:test_len]\n",
    "    train_user_list = list(p_vals.keys())[test_len:]\n",
    "\n",
    "    print('test users:', len(test_user_list))\n",
    "    print('train users:', len(train_user_list))\n",
    "\n",
    "    if args.use_demo:\n",
    "        # get demo mapping\n",
    "        meta_df = pd.read_csv(get_file_from_s3(f\"human_resp/{args.survey_name}/metadata.csv\"))\n",
    "        meta_keys = list(meta_df['key'])\n",
    "\n",
    "\n",
    "    def get_user_pval_df(pvals, user_list):\n",
    "        \n",
    "        records = []\n",
    "        for user in user_list:\n",
    "            personas = pvals[user]\n",
    "            entry = {'user': user}\n",
    "            for p in personas:\n",
    "                entry[p['name']] = p['inferred_value']\n",
    "\n",
    "            records.append(entry)\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        return df\n",
    "\n",
    "\n",
    "    df = get_user_pval_df(p_vals, train_user_list)\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    skew_cnt = 0\n",
    "    skew_personas = []\n",
    "    for col in df.columns:\n",
    "        if col == 'user': continue\n",
    "        cnt = Counter(df[col].values)\n",
    "        if len(cnt.keys()) < 2: \n",
    "            skew_cnt += 1\n",
    "            skew_personas.append(col) \n",
    "            print(col)\n",
    "            print(Counter(df[col].values))\n",
    "            print()\n",
    "            continue\n",
    "        first, second = cnt.most_common(2)\n",
    "        if first[1] / second[1] > args.skew:\n",
    "            skew_cnt += 1\n",
    "            skew_personas.append(col) \n",
    "            print(col)\n",
    "            print(Counter(df[col].values))\n",
    "            print()\n",
    "    print(skew_cnt / len(df.columns))\n",
    "\n",
    "\n",
    "    su = test_user_list[0]\n",
    "\n",
    "    def get_user_persona_repr(user, use_demo):\n",
    "        personas = p_vals[user]\n",
    "        if use_demo:\n",
    "            row = resp_df.iloc[int(user)]\n",
    "            # get demo\n",
    "            demo = row[meta_keys].to_dict()\n",
    "            key_mappings = {\n",
    "                'CREGION': 'Region Where The Participant Lives',\n",
    "                'AGE': 'Age Range Of The Participant',\n",
    "                'SEX': 'Gender Of The Participant',\n",
    "                'EDUCATION': 'Educational Attainment Of The Participant',\n",
    "                'CITIZEN': 'Citizenship Status Of The Participant',\n",
    "                'MARITAL': 'Marital Status Of The Participant',\n",
    "                'RELIG': 'Religious Affiliation Of The Participant',\n",
    "                'RELIGATTEND': 'Frequency Of Religious Service Attendance',\n",
    "                'POLPARTY': 'Political Party Affiliation',\n",
    "                'INCOME': 'Annual Income Range',\n",
    "                'POLIDEOLOGY': 'Political Ideology',\n",
    "                'RACE': 'Racial Background'\n",
    "            }\n",
    "            demo = {key_mappings[k]: v for k, v in demo.items()}\n",
    "            for k, v in demo.items():\n",
    "                personas.append({\n",
    "                    'name': k,\n",
    "                    'inferred_value': v\n",
    "                })\n",
    "\n",
    "        # return '; '.join(f\"{p['name']}: {p['inferred_value']}\" for p in personas)\n",
    "        return '; '.join(f\"{p['name']}: {p['inferred_value']}\" for p in personas if p['name'] not in skew_personas)\n",
    "\n",
    "    print(get_user_persona_repr(su, args.use_demo))\n",
    "\n",
    "\n",
    "\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import time\n",
    "\n",
    "    # Each query needs to be accompanied by an corresponding instruction describing the task.\n",
    "    task_name_to_instruct = {\"retrieve_user_personas\": \"Given a user persona description, retrieve similar user presona descriptions\",}\n",
    "\n",
    "    query_prefix = \"Instruct: \"+task_name_to_instruct[\"retrieve_user_personas\"]+\"\\nQuery: \"\n",
    "    queries = [\n",
    "        get_user_persona_repr(user, args.use_demo) for user in test_user_list\n",
    "    ]\n",
    "\n",
    "    # No instruction needed for retrieval passages\n",
    "    passage_prefix = \"\"\n",
    "    passages = [\n",
    "        get_user_persona_repr(user, args.use_demo) for user in train_user_list[:args.train_size]\n",
    "    ]\n",
    "\n",
    "    # load model with tokenizer\n",
    "    model = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True, device_map='auto')\n",
    "\n",
    "    # get the embeddings\n",
    "    max_length = 4096\n",
    "    tic = time.time()\n",
    "    # query_embeddings = model.encode(queries, instruction=query_prefix, max_length=max_length)\n",
    "    # passage_embeddings = model.encode(passages, instruction=passage_prefix, max_length=max_length)\n",
    "    batch_size=2\n",
    "    query_embeddings = model._do_encode(queries, batch_size=batch_size, instruction=query_prefix, max_length=max_length, num_workers=32, return_numpy=True)\n",
    "    toc = time.time()\n",
    "    print('Test encoding time:', toc-tic)\n",
    "\n",
    "    passage_embeddings = model._do_encode(passages, batch_size=batch_size, instruction=passage_prefix, max_length=max_length, num_workers=32, return_numpy=True)\n",
    "    toc = time.time()\n",
    "    print('Total encoding time:', toc-tic)\n",
    "    query_embeddings = F.normalize(torch.tensor(query_embeddings), p=2, dim=1)\n",
    "    passage_embeddings = F.normalize(torch.tensor(passage_embeddings), p=2, dim=1)\n",
    "\n",
    "    scores = (query_embeddings @ passage_embeddings.T) * 100\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    def get_top_k_similar_users(scores_matrix, query_user_names, corpus_user_names, k):\n",
    "        top_k_similar_users = {}\n",
    "        scores_matrix = scores_matrix.detach().cpu().numpy()\n",
    "\n",
    "        for i, curr_scores in enumerate(scores_matrix):\n",
    "            # Get the indices of the top k scores\n",
    "            top_k_indices = np.argsort(curr_scores)[-k:][::-1]\n",
    "            \n",
    "            # Map indices to corresponding user names in the corpus_user_names list\n",
    "            top_k_users = [corpus_user_names[idx] for idx in top_k_indices]\n",
    "            \n",
    "            # Map the query user name to the list of top k similar users\n",
    "            top_k_similar_users[query_user_names[i]] = top_k_users\n",
    "\n",
    "        return top_k_similar_users\n",
    "\n",
    "\n",
    "    result = get_top_k_similar_users(scores, test_user_list, train_user_list, args.top_k)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--survey_name', type=str)\n",
    "    parser.add_argument('--persona_val_path', type=str)\n",
    "    parser.add_argument('--user_mapping_filepath', type=str)\n",
    "    parser.add_argument('--train_size', type=int)\n",
    "    parser.add_argument('--top_k', type=int)\n",
    "    parser.add_argument('--skew', type=int)\n",
    "    parser.add_argument('--use_demo', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.chdir('/home/ubuntu/code/pump_post_midterm/pump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hp_grid_search_v2.json') as f:\n",
    "    search = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------+--------------+----------+\n",
      "|    | setting      |   top_k |   skew_thres |   result |\n",
      "|----+--------------+---------+--------------+----------|\n",
      "| 15 | demo_persona |     250 |            5 | 0.590909 |\n",
      "| 50 | persona_only |     200 |            3 | 0.585639 |\n",
      "| 45 | persona_only |     100 |            3 | 0.58498  |\n",
      "| 14 | demo_persona |     200 |            5 | 0.58498  |\n",
      "| 47 | persona_only |     140 |            3 | 0.584321 |\n",
      "| 13 | demo_persona |     180 |            5 | 0.582345 |\n",
      "| 22 | demo_persona |     180 |            3 | 0.581686 |\n",
      "| 10 | demo_persona |     120 |            5 | 0.581028 |\n",
      "| 21 | demo_persona |     160 |            3 | 0.580369 |\n",
      "| 16 | demo_persona |     300 |            5 | 0.57971  |\n",
      "| 25 | demo_persona |     300 |            3 | 0.57971  |\n",
      "|  7 | demo_persona |     300 |           10 | 0.57971  |\n",
      "|  4 | demo_persona |     180 |           10 | 0.57971  |\n",
      "|  3 | demo_persona |     160 |           10 | 0.57971  |\n",
      "| 11 | demo_persona |     140 |            5 | 0.579051 |\n",
      "|  9 | demo_persona |     100 |            5 | 0.578393 |\n",
      "| 46 | persona_only |     120 |            3 | 0.578393 |\n",
      "| 49 | persona_only |     180 |            3 | 0.577734 |\n",
      "| 26 | demo_persona |     350 |            3 | 0.577075 |\n",
      "|  6 | demo_persona |     250 |           10 | 0.576416 |\n",
      "|  5 | demo_persona |     200 |           10 | 0.576416 |\n",
      "| 19 | demo_persona |     120 |            3 | 0.576416 |\n",
      "|  0 | demo_persona |     100 |           10 | 0.576416 |\n",
      "| 68 | demo_only    |     200 |            5 | 0.575758 |\n",
      "| 59 | demo_only    |     200 |           10 | 0.575758 |\n",
      "| 28 | persona_only |     120 |           10 | 0.575758 |\n",
      "| 77 | demo_only    |     200 |            3 | 0.575758 |\n",
      "| 17 | demo_persona |     350 |            5 | 0.575099 |\n",
      "| 48 | persona_only |     160 |            3 | 0.575099 |\n",
      "| 23 | demo_persona |     200 |            3 | 0.575099 |\n",
      "| 24 | demo_persona |     250 |            3 | 0.575099 |\n",
      "| 12 | demo_persona |     160 |            5 | 0.57444  |\n",
      "|  2 | demo_persona |     140 |           10 | 0.57444  |\n",
      "| 52 | persona_only |     300 |            3 | 0.573781 |\n",
      "|  1 | demo_persona |     120 |           10 | 0.573781 |\n",
      "| 51 | persona_only |     250 |            3 | 0.573123 |\n",
      "|  8 | demo_persona |     350 |           10 | 0.573123 |\n",
      "| 38 | persona_only |     140 |            5 | 0.572464 |\n",
      "| 37 | persona_only |     120 |            5 | 0.572464 |\n",
      "| 69 | demo_only    |     250 |            5 | 0.571805 |\n",
      "| 78 | demo_only    |     250 |            3 | 0.571805 |\n",
      "| 60 | demo_only    |     250 |           10 | 0.571805 |\n",
      "| 74 | demo_only    |     140 |            3 | 0.571146 |\n",
      "| 56 | demo_only    |     140 |           10 | 0.571146 |\n",
      "| 65 | demo_only    |     140 |            5 | 0.571146 |\n",
      "| 27 | persona_only |     100 |           10 | 0.571146 |\n",
      "| 66 | demo_only    |     160 |            5 | 0.570487 |\n",
      "| 67 | demo_only    |     180 |            5 | 0.570487 |\n",
      "| 57 | demo_only    |     160 |           10 | 0.570487 |\n",
      "| 58 | demo_only    |     180 |           10 | 0.570487 |\n",
      "| 44 | persona_only |     350 |            5 | 0.570487 |\n",
      "| 20 | demo_persona |     140 |            3 | 0.570487 |\n",
      "| 39 | persona_only |     160 |            5 | 0.570487 |\n",
      "| 75 | demo_only    |     160 |            3 | 0.570487 |\n",
      "| 76 | demo_only    |     180 |            3 | 0.570487 |\n",
      "| 35 | persona_only |     350 |           10 | 0.56917  |\n",
      "| 70 | demo_only    |     300 |            5 | 0.568511 |\n",
      "| 41 | persona_only |     200 |            5 | 0.568511 |\n",
      "| 33 | persona_only |     250 |           10 | 0.568511 |\n",
      "| 79 | demo_only    |     300 |            3 | 0.568511 |\n",
      "| 61 | demo_only    |     300 |           10 | 0.568511 |\n",
      "| 18 | demo_persona |     100 |            3 | 0.567852 |\n",
      "| 40 | persona_only |     180 |            5 | 0.567194 |\n",
      "| 30 | persona_only |     160 |           10 | 0.567194 |\n",
      "| 29 | persona_only |     140 |           10 | 0.567194 |\n",
      "| 42 | persona_only |     250 |            5 | 0.567194 |\n",
      "| 36 | persona_only |     100 |            5 | 0.567194 |\n",
      "| 43 | persona_only |     300 |            5 | 0.566535 |\n",
      "| 80 | demo_only    |     350 |            3 | 0.565876 |\n",
      "| 62 | demo_only    |     350 |           10 | 0.565876 |\n",
      "| 55 | demo_only    |     120 |           10 | 0.565876 |\n",
      "| 53 | persona_only |     350 |            3 | 0.565876 |\n",
      "| 71 | demo_only    |     350 |            5 | 0.565876 |\n",
      "| 64 | demo_only    |     120 |            5 | 0.565876 |\n",
      "| 73 | demo_only    |     120 |            3 | 0.565876 |\n",
      "| 34 | persona_only |     300 |           10 | 0.565217 |\n",
      "| 32 | persona_only |     200 |           10 | 0.565217 |\n",
      "| 31 | persona_only |     180 |           10 | 0.564559 |\n",
      "| 63 | demo_only    |     100 |            5 | 0.555995 |\n",
      "| 54 | demo_only    |     100 |           10 | 0.555995 |\n",
      "| 72 | demo_only    |     100 |            3 | 0.555995 |\n",
      "+----+--------------+---------+--------------+----------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "df = pd.DataFrame(search)\n",
    "\n",
    "df.sort_values(by='result', ascending=False)\n",
    "\n",
    "subdf = df[['setting', 'top_k', 'skew_thres', 'result']].sort_values(by='result', ascending=False)\n",
    "print(tabulate.tabulate(subdf, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_mapping = {\n",
    "    \"demo_persona\": \"querydp\",\n",
    "    \"demo_only\": \"querydonly\",\n",
    "    \"persona_only\": \"queryponly\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setting                                                      demo_persona\n",
       "skew_thres                                                              3\n",
       "top_k                                                                  40\n",
       "result                                                            0.57971\n",
       "similar_user_mapping    {'1622': ['77', '1314', '1283', '70', '1426', ...\n",
       "Name: 23, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.iloc[23]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '0902'\n",
    "\n",
    "mapping_name = f\"date{date}_personas_full_haiku_known_test_{setting_mapping[row['setting']]}_trainAll_top{row['top_k']}_skew{row['skew_thres']}_withname.json\"\n",
    "\n",
    "with open(f'opinions_qa/similar_users/American_Trends_Panel_W34/{mapping_name}', 'w') as f:\n",
    "    json.dump(row['similar_user_mapping'], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setting                                                      persona_only\n",
       "skew_thres                                                             10\n",
       "top_k                                                                  70\n",
       "result                                                           0.583004\n",
       "similar_user_mapping    {'1622': ['1914', '201', '2272', '1921', '908'...\n",
       "Name: 36, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.iloc[36]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '0902'\n",
    "\n",
    "mapping_name = f\"date{date}_personas_full_haiku_known_test_{setting_mapping[row['setting']]}_trainAll_top{row['top_k']}_skew{row['skew_thres']}_withname.json\"\n",
    "\n",
    "with open(f'opinions_qa/similar_users/American_Trends_Panel_W34/{mapping_name}', 'w') as f:\n",
    "    json.dump(row['similar_user_mapping'], f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
